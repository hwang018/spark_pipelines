{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General pipeline for binary classification tasks in spark ml\n",
    "\n",
    "$\\color{blue}{\\text{Covering major components in real life scenarios for binary classification tasks using structured datasets.}}$\n",
    "\n",
    "Modelling and data transformation part will be mostly operated in pure spark, but exploration and plotting metrics may involve pandas components.\n",
    "\n",
    "The toolkits are in the lib/ folder, covering following topics:\n",
    "\n",
    "0. summary on transformer, estimators, pipelines\n",
    "1. spark and pandas dataframe conversion, tips in converting datatypes and assign correct schema\n",
    "2. categorical variables encoding methods: label encoding (string indexer), one hot encoding.\n",
    "3. feature selection methods in spark ml, selection based on random forest.\n",
    "4. handling skewed datasets and highly imbalanced labels (up/down sampling) SMOTE in spark\n",
    "5. modelling toolkits, contains common classifiers and their tuning guidance, use of xgboost in spark\n",
    "6. metrics plotting tools, to plot common metrics after training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark ml structure:\n",
    "Key components:\n",
    "1. Transformer\n",
    "2. Estimator\n",
    "3. Pipeline\n",
    "\n",
    "$\\textbf{Transformer}$ can transform one df into another df by appending new columns onto original df. E.g., an ML model is a Transformer which transforms a DataFrame with features into a DataFrame with predictions. It has .transform() method, normally taking df as input. Transformers can be trained models, trained encoders.\n",
    "\n",
    "$\\textbf{Estimator}$ is an algorithm to be fit on a df to produce a Transformer. E.g., a learning algorithm is an Estimator which trains on a df and produces a model; if we specify a One-hot-encoder, it is an estimarot object, we need to .fit() it onto a column and obtain a transformer. Output of fitted/trained estimator is transformer.\n",
    "\n",
    "$\\textbf{Pipeline}$ chains multiple Transformers and Estimators together to specify an ML workflow. When executing the pipeline, spark will automatically sort out the steps to execute, depending on whether you called a .fit() or .transform() method. A Pipeline is specified as a sequence of stages, and each stage is either a Transformer or an Estimator. These stages are run in order, and the input DataFrame is transformed as it passes through each stage. For Transformer stages, the transform() method is called on the DataFrame. For Estimator stages, the fit() method is called to produce a Transformer (which becomes part of the PipelineModel, or fitted Pipeline). Pipeline is an estimator, after calling pipeline.fit() method, the output will be PipelineModel, a transformer ready to apply .transform() at test time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "pd.options.display.max_columns=None\n",
    "pd.options.display.max_rows=None\n",
    "\n",
    "#import toolkits\n",
    "from lib import util\n",
    "from lib import logger\n",
    "\n",
    "def initialize_spark(app_name='spark_pipeline'):\n",
    "    import findspark\n",
    "    #spark path using default value\n",
    "    findspark.init()\n",
    "\n",
    "    import pyspark\n",
    "    import pyarrow\n",
    "    from pyspark.sql import SQLContext\n",
    "    \n",
    "    #broadcastTimeout is purposedly set to be large due to development on single machine\n",
    "    conf = pyspark.SparkConf()\\\n",
    "        .setAppName(app_name)\\\n",
    "        .setMaster('local')\\\n",
    "        .set('spark.driver.memory', '8g')\\\n",
    "        .set('spark.executor.memory', '8g')\\\n",
    "        .set('spark.executor.instances', 4)\\\n",
    "        .set('spark.executor.cores', 4)\\\n",
    "        .set('spark.driver.maxResultSize', '8g')\\\n",
    "        .set('spark.sql.shuffle.partitions', 100)\\\n",
    "        .set('spark.default.parallelism', 200)\\\n",
    "        .set('spark.sql.broadcastTimeout', 36000)\\\n",
    "        .set('spark.kryoserializer.buffer.max', '1024m')\\\n",
    "        .set('spark.sql.execution.arrow.enabled', 'false')\\\n",
    "        .set('spark.dynamicAllocation.enabled', \"False\")\\\n",
    "        .set('spark.port.maxRetries',30) \n",
    "\n",
    "    sc = pyspark.SparkContext.getOrCreate(conf)\n",
    "    spark = pyspark.sql.SparkSession(sc)\n",
    "    sqlContext = SQLContext.getOrCreate(sc)    \n",
    "    return sc,spark,sqlContext\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml.feature import OneHotEncoderEstimator, StringIndexer, VectorAssembler\n",
    "from pyspark.sql.types import IntegerType,DecimalType\n",
    "from pyspark.sql.functions import when, lit\n",
    "from distutils.version import LooseVersion\n",
    "from importlib import reload\n",
    "import pyspark.sql.functions as func\n",
    "import pyspark.sql.types as typ\n",
    "\n",
    "from config.conf_template import Struct as Section\n",
    "from pyspark.ml.feature import OneHotEncoderEstimator\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "import lib.imbalance_handler as imbalance_handle\n",
    "import lib.feature_selection as fs\n",
    "import lib.categorical_handler as ctgy\n",
    "import lib.modelling as model\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "reload(ctgy)\n",
    "reload(fs)\n",
    "reload(imbalance_handle)\n",
    "reload(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc,spark,sqlContext = initialize_spark()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Load data into spark df, stringIndex all cat_cols\n",
    "\n",
    "Stringindex all -> train test split -> smote train -> restore smoted train to original columns -> train models -> transform testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('datasets/adult.csv')\n",
    "# if directly using spark.read.csv('datasets/adult.csv',header=True), unless we specify schema manually,\n",
    "# all columns will be interpreted as string type, troublesome for later process\n",
    "dataset = util.pandas_to_spark(sqlContext,df)\n",
    "dataset = dataset.withColumn('income', when(dataset.income=='<=50K', lit(0)).otherwise(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All columns are been covered.\n"
     ]
    }
   ],
   "source": [
    "# get num_cols and cat_cols from spark df\n",
    "num_cols, cat_cols = util.get_num_cat_feat(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingData,testData = dataset.randomSplit([0.7, 0.3], seed=100)\n",
    "# manual rename, target col to 'label' on test set, train set will auto convert\n",
    "testData = testData.withColumnRenamed(\"income\",\"label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) Random downsampling, for highly imbalanced dataset\n",
    "Example usage of downsampling spark df:\n",
    "down_sampled_df = imbalance_handle.spark_df_down_sampling(trainingData, 2, 'income', major_class_val = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Smote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = Section(\"smote_config\")\n",
    "conf.seed = 48\n",
    "conf.bucketLength = 100\n",
    "conf.k = 4\n",
    "conf.multiplier = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "return num cols vectorized df and stages for testset transformation\n"
     ]
    }
   ],
   "source": [
    "vectorized,smote_stages = imbalance_handle.pre_smote_df_process(trainingData,num_cols,cat_cols,'income',False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating batch 0 of synthetic instances\n",
      "generating batch 1 of synthetic instances\n",
      "generating batch 2 of synthetic instances\n"
     ]
    }
   ],
   "source": [
    "smoted_train_df = imbalance_handle.smote(vectorized,conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_restored = imbalance_handle.restore_smoted_df(num_cols,smoted_train_df,'features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Feature selection using RF, for high number of features use cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_settings = {'maxBins':100,\n",
    "              'labelCol':'label',\n",
    "              'featuresCol':'features'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preparing stages to prepare for rf input vectorized df\n",
      "transforming df (label encoding and vectorizing)\n",
      "fitting random forest model using maxBins:100\n",
      "return top 10 features from rf feature importance\n"
     ]
    }
   ],
   "source": [
    "# consider dropping high category cat cols before rf, the max cardinality should be less than maxBins\n",
    "features_selected = fs.RF_feature_selector(rf_settings,res_restored,num_cols,cat_cols,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Encode categorical cols (optional for tree based algo)\n",
    "\n",
    "    with 1hot encoding on cat cols, output string columns will be with suffix _index_ohe\n",
    "    allstages = ctgy.assemble_into_features(res_restored,num_cols,cat_cols,'_index','_ohe')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Model construction, tuned RF and GBT\n",
    "\n",
    "    GBTs train one tree at a time, so they can take longer to train than random forests. Random Forests can train multiple trees in parallel.\n",
    "    On the other hand, it is often reasonable to use smaller (shallower) trees with GBTs than with Random Forests, and training smaller trees takes less time.\n",
    "    Random Forests can be less prone to overfitting. Training more trees in a Random Forest reduces the likelihood of overfitting, but training more trees with GBTs increases the likelihood of overfitting. (In statistical language, Random Forests reduce variance by using more trees, whereas GBTs reduce bias by using more trees.)\n",
    "    Random Forests can be easier to tune since performance improves monotonically with the number of trees (whereas performance can start to decrease for GBTs if the number of trees grows too large)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'RF':{'maxDepth':[4,6],'maxBins':[80],'numTrees':[20]},\n",
    "             'GBT':{'maxDepth':[6],'maxBins':[80]}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "received rf param grid: {'maxDepth': [4, 6], 'maxBins': [80], 'numTrees': [20]}\n",
      "transforming train df\n",
      "transforming test df\n",
      "fitting cv rf models on transformed train df\n",
      "best model performance on test set:\n",
      "AUC: 0.8920428449542431, AUPR: 0.7162176681645379\n",
      "return best rf model\n",
      "CPU times: user 3.25 s, sys: 1.13 s, total: 4.38 s\n",
      "Wall time: 14min 40s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "Best_RF = model.train_rf_param_tuning(params,res_restored,testData,num_cols,cat_cols,cv_Folds=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
