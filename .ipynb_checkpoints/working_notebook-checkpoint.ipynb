{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General pipeline for binary classification tasks in spark ml\n",
    "\n",
    "$\\color{blue}{\\text{Covering major components in real life scenarios for binary classification tasks using structured datasets.}}$\n",
    "\n",
    "Modelling and data transformation part will be mostly operated in pure spark, but exploration and plotting metrics may involve pandas components.\n",
    "\n",
    "The toolkits are in the lib/ folder, covering following topics:\n",
    "\n",
    "0. summary on transformer, estimators, pipelines\n",
    "1. spark and pandas dataframe conversion, tips in converting datatypes and assign correct schema\n",
    "2. categorical variables encoding methods: label encoding (string indexer), one hot encoding.\n",
    "3. feature selection methods in spark ml, selection based on random forest.\n",
    "4. handling skewed datasets and highly imbalanced labels (up/down sampling) SMOTE in spark\n",
    "5. modelling toolkits, contains common classifiers and their tuning guidance, use of xgboost in spark\n",
    "6. metrics plotting tools, to plot common metrics after training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark ml structure:\n",
    "Key components:\n",
    "1. Transformer\n",
    "2. Estimator\n",
    "3. Pipeline\n",
    "\n",
    "$\\textbf{Transformer}$ can transform one df into another df by appending new columns onto original df. E.g., an ML model is a Transformer which transforms a DataFrame with features into a DataFrame with predictions. It has .transform() method, normally taking df as input. Transformers can be trained models, trained encoders.\n",
    "\n",
    "$\\textbf{Estimator}$ is an algorithm to be fit on a df to produce a Transformer. E.g., a learning algorithm is an Estimator which trains on a df and produces a model; if we specify a One-hot-encoder, it is an estimarot object, we need to .fit() it onto a column and obtain a transformer. Output of fitted/trained estimator is transformer.\n",
    "\n",
    "$\\textbf{Pipeline}$ chains multiple Transformers and Estimators together to specify an ML workflow. When executing the pipeline, spark will automatically sort out the steps to execute, depending on whether you called a .fit() or .transform() method. A Pipeline is specified as a sequence of stages, and each stage is either a Transformer or an Estimator. These stages are run in order, and the input DataFrame is transformed as it passes through each stage. For Transformer stages, the transform() method is called on the DataFrame. For Estimator stages, the fit() method is called to produce a Transformer (which becomes part of the PipelineModel, or fitted Pipeline). Pipeline is an estimator, after calling pipeline.fit() method, the output will be PipelineModel, a transformer ready to apply .transform() at test time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "pd.options.display.max_columns=None\n",
    "pd.options.display.max_rows=None\n",
    "\n",
    "#import toolkits\n",
    "from lib import util\n",
    "from lib import logger\n",
    "\n",
    "def initialize_spark(app_name='spark_pipeline'):\n",
    "    import findspark\n",
    "    #spark path using default value\n",
    "    findspark.init()\n",
    "\n",
    "    import pyspark\n",
    "    import pyarrow\n",
    "    from pyspark.sql import SQLContext\n",
    "    \n",
    "    #broadcastTimeout is purposedly set to be large due to development on single machine\n",
    "    conf = pyspark.SparkConf()\\\n",
    "        .setAppName(app_name)\\\n",
    "        .setMaster('local')\\\n",
    "        .set('spark.driver.memory', '8g')\\\n",
    "        .set('spark.executor.memory', '8g')\\\n",
    "        .set('spark.executor.instances', 4)\\\n",
    "        .set('spark.executor.cores', 4)\\\n",
    "        .set('spark.driver.maxResultSize', '8g')\\\n",
    "        .set('spark.sql.shuffle.partitions', 100)\\\n",
    "        .set('spark.default.parallelism', 200)\\\n",
    "        .set('spark.sql.broadcastTimeout', 36000)\\\n",
    "        .set('spark.kryoserializer.buffer.max', '1024m')\\\n",
    "        .set('spark.sql.execution.arrow.enabled', 'false')\\\n",
    "        .set('spark.dynamicAllocation.enabled', \"False\")\\\n",
    "        .set('spark.port.maxRetries',30) \n",
    "\n",
    "    sc = pyspark.SparkContext.getOrCreate(conf)\n",
    "    spark = pyspark.sql.SparkSession(sc)\n",
    "    sqlContext = SQLContext.getOrCreate(sc)    \n",
    "    return sc,spark,sqlContext\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml.feature import OneHotEncoderEstimator, StringIndexer, VectorAssembler\n",
    "from pyspark.sql.types import IntegerType,DecimalType\n",
    "from pyspark.sql.functions import when, lit\n",
    "from distutils.version import LooseVersion\n",
    "from importlib import reload\n",
    "import pyspark.sql.functions as func\n",
    "import pyspark.sql.types as typ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc,spark,sqlContext = initialize_spark()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Load data into spark df, stringIndex all cat_cols\n",
    "\n",
    "Stringindex all -> train test split -> smote train -> restore smoted train to original columns -> train models -> transform testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('datasets/adult.csv')\n",
    "# if directly using spark.read.csv('datasets/adult.csv',header=True), unless we specify schema manually,\n",
    "# all columns will be interpreted as string type, troublesome for later process\n",
    "dataset = util.pandas_to_spark(sqlContext,df)\n",
    "dataset = dataset.withColumn('income', when(dataset.income=='<=50K', lit(0)).otherwise(1))\n",
    "cols = dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingData,testData = dataset.randomSplit([0.7, 0.3], seed=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) Random downsampling, for highly imbalanced dataset\n",
    "Example usage of downsampling spark df:\n",
    "down_sampled_df = imbalance_handle.spark_df_down_sampling(trainingData, 2, 'income', major_class_val = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config.conf_template import Struct as Section\n",
    "from pyspark.ml.feature import OneHotEncoderEstimator\n",
    "import lib.imbalance_handler as imbalance_handle\n",
    "import lib.feature_selection as fs\n",
    "import lib.categorical_handler as ctgy\n",
    "from pyspark.ml import Pipeline\n",
    "reload(ctgy)\n",
    "reload(fs)\n",
    "reload(imbalance_handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Smote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = Section(\"smote_config\")\n",
    "conf.seed = 48\n",
    "conf.bucketLength = 100\n",
    "conf.k = 4\n",
    "conf.multiplier = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get num_cols and cat_cols from spark df\n",
    "num_cols, cat_cols = util.get_num_cat_feat(dataset)\n",
    "vectorized,stages1 = imbalance_handle.pre_smote_df_process(trainingData,num_cols,cat_cols,'income',False)\n",
    "res = imbalance_handle.smote(vectorized,conf)\n",
    "res_restored = restore_smoted_df(num_cols,res,'features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Feature selection using RF, for high number of features use cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now to train on the train set\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "import numpy as np\n",
    "\n",
    "rf_settings = {'maxBins':100,\n",
    "              'labelCol':'label',\n",
    "              'featuresCol':'features'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# consider dropping high category cat cols before rf, the max cardinality should be less than maxBins\n",
    "fs.RF_feature_selector(rf_settings,res_restored,num_cols,cat_cols,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Encode categorical cols\n",
    "\n",
    "    with 1hot encoding on cat cols, output string columns will be with suffix _index_ohe\n",
    "    allstages = ctgy.assemble_into_features(res_restored,num_cols,cat_cols,'_index','_ohe')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Model construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with label encoding on cat cols\n",
    "stages_rf = ctgy.assemble_into_features_RF(res_restored,num_cols,cat_cols,'_index')\n",
    "# with label encoding on cat cols and one hot encode them\n",
    "stages_ohe = ctgy.assemble_into_features_OHE(res_restored,num_cols,cat_cols,'_index','_ohe')\n",
    "\n",
    "#having compiled the stages into a list, at execution, it will automatically sort out the sequence to perform steps in stages\n",
    "partialPipeline = Pipeline().setStages(stages_rf) #type is pipeline, independent of dataframe, only using stages \n",
    "pipelineModel = partialPipeline.fit(res_restored) #type is pipelinemodel, use the prepared staged pipelines to transform test dataframe or train\n",
    "preppedDataDF = pipelineModel.transform(res_restored) #type is stage transformed dataframe, it contains all original columns, and indexed/encoded/vector_encoded columns\n",
    "\n",
    "preppedDataDF_test = pipelineModel.transform(testData)\n",
    "preppedDataDF_test = preppedDataDF_test.withColumnRenamed(\"income\",\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now to train on the train set\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "# Create an initial RandomForest model.\n",
    "rf = RandomForestClassifier(maxBins=100, labelCol=\"label\", featuresCol=\"features\")\n",
    "# Train model with Training Data\n",
    "rfModel = rf.fit(preppedDataDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on test data using the transform() method.\n",
    "# LogisticRegression.transform() will only use the 'features' column.\n",
    "predictions = rfModel.transform(preppedDataDF_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "# Evaluate model\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\n",
    "areaUnderROC = evaluator.setMetricName(\"areaUnderROC\").evaluate(predictions)\n",
    "areaUnderPR = evaluator.setMetricName(\"areaUnderPR\").evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = predictions.select(\"prediction\",\"label\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_pred=preds['prediction'],y_true=preds['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(rf.maxDepth, [2, 4, 6])\n",
    "             .addGrid(rf.maxBins, [20, 60])\n",
    "             .addGrid(rf.numTrees, [5, 20])\n",
    "             .build())\n",
    "\n",
    "# paramGrid contains 3*2*2 = 12 models\n",
    "# cv is 5 folds, so total 60 models are searched\n",
    "\n",
    "# Create 5-fold CrossValidator, input is an estimator (rf classifier e.g.)\n",
    "cv = CrossValidator(estimator=rf, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=5)\n",
    "# Run cross validations\n",
    "cvModel = cv.fit(trainingData)\n",
    "# this will likely take a fair amount of time because of the amount of models that we're creating and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use test set to measure the accuracy of our model on new data\n",
    "predictions = cvModel.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cvModel uses the best model found from the Cross Validation\n",
    "# Evaluate best model\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector assembler can have inputs as: numeric,bool,vector\n",
    "# output will be a flattened vector (even if input could have vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestModel = cvModel.bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions for entire dataset\n",
    "finalPredictions = bestModel.transform(dataset)\n",
    "# Evaluate best model\n",
    "evaluator.evaluate(finalPredictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## trash code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "from pyspark.ml.feature import StringIndexer, IndexToString\n",
    "labelReverse = IndexToString().setInputCol(\"race_index\").setOutputCol(\"recover\")\n",
    "labelReverse.transform(vectorized).select(\"race_index\",\"recover\").show()\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
