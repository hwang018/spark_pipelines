{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General pipeline for binary classification tasks in spark ml\n",
    "\n",
    "$\\color{blue}{\\text{Covering major components in real life scenarios for binary classification tasks using structured datasets.}}$\n",
    "\n",
    "Modelling and data transformation part will be mostly operated in pure spark, but exploration and plotting metrics may involve pandas components.\n",
    "\n",
    "The toolkits are in the lib/ folder, covering following topics:\n",
    "\n",
    "0. summary on transformer, estimators, pipelines\n",
    "1. spark and pandas dataframe conversion, tips in converting datatypes and assign correct schema\n",
    "2. categorical variables encoding methods: label encoding (string indexer), one hot encoding.\n",
    "3. feature selection methods in spark ml, selection based on model, lasso.\n",
    "4. handling skewed datasets and highly imbalanced labels (up/down sampling) SMOTE in spark\n",
    "5. modelling toolkits, contains common classifiers and their tuning guidance, use of xgboost in spark\n",
    "6. metrics plotting tools, to plot common metrics after training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark ml structure:\n",
    "Key components:\n",
    "1. Transformer\n",
    "2. Estimator\n",
    "3. Pipeline\n",
    "\n",
    "$\\textbf{Transformer}$ can transform one df into another df by appending new columns onto original df. E.g., an ML model is a Transformer which transforms a DataFrame with features into a DataFrame with predictions. It has .transform() method, normally taking df as input. Transformers can be trained models, trained encoders.\n",
    "\n",
    "$\\textbf{Estimator}$ is an algorithm to be fit on a df to produce a Transformer. E.g., a learning algorithm is an Estimator which trains on a df and produces a model; if we specify a One-hot-encoder, it is an estimarot object, we need to .fit() it onto a column and obtain a transformer. Output of fitted/trained estimator is transformer.\n",
    "\n",
    "$\\textbf{Pipeline}$ chains multiple Transformers and Estimators together to specify an ML workflow. When executing the pipeline, spark will automatically sort out the steps to execute, depending on whether you called a .fit() or .transform() method. A Pipeline is specified as a sequence of stages, and each stage is either a Transformer or an Estimator. These stages are run in order, and the input DataFrame is transformed as it passes through each stage. For Transformer stages, the transform() method is called on the DataFrame. For Estimator stages, the fit() method is called to produce a Transformer (which becomes part of the PipelineModel, or fitted Pipeline). Pipeline is an estimator, after calling pipeline.fit() method, the output will be PipelineModel, a transformer ready to apply .transform() at test time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "pd.options.display.max_columns=None\n",
    "pd.options.display.max_rows=None\n",
    "\n",
    "#import toolkits\n",
    "from lib import util\n",
    "from lib import logger\n",
    "\n",
    "def initialize_spark(app_name='spark_pipeline'):\n",
    "    import findspark\n",
    "    #spark path using default value\n",
    "    findspark.init()\n",
    "\n",
    "    import pyspark\n",
    "    import pyarrow\n",
    "    from pyspark.sql import SQLContext\n",
    "    \n",
    "    #broadcastTimeout is purposedly set to be large due to development on single machine\n",
    "    conf = pyspark.SparkConf()\\\n",
    "        .setAppName(app_name)\\\n",
    "        .setMaster('local')\\\n",
    "        .set('spark.driver.memory', '8g')\\\n",
    "        .set('spark.executor.memory', '8g')\\\n",
    "        .set('spark.executor.instances', 4)\\\n",
    "        .set('spark.executor.cores', 4)\\\n",
    "        .set('spark.driver.maxResultSize', '8g')\\\n",
    "        .set('spark.sql.shuffle.partitions', 100)\\\n",
    "        .set('spark.default.parallelism', 200)\\\n",
    "        .set('spark.sql.broadcastTimeout', 36000)\\\n",
    "        .set('spark.kryoserializer.buffer.max', '1024m')\\\n",
    "        .set('spark.sql.execution.arrow.enabled', 'false')\\\n",
    "        .set('spark.dynamicAllocation.enabled', \"False\")\\\n",
    "        .set('spark.port.maxRetries',30) \n",
    "\n",
    "    sc = pyspark.SparkContext.getOrCreate(conf)\n",
    "    spark = pyspark.sql.SparkSession(sc)\n",
    "    sqlContext = SQLContext.getOrCreate(sc)    \n",
    "    return sc,spark,sqlContext\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml.feature import OneHotEncoderEstimator, StringIndexer, VectorAssembler\n",
    "from pyspark.sql.types import IntegerType,DecimalType\n",
    "from pyspark.sql.functions import when, lit\n",
    "from distutils.version import LooseVersion\n",
    "from importlib import reload\n",
    "import pyspark.sql.functions as func\n",
    "import pyspark.sql.types as typ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc,spark,sqlContext = initialize_spark()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Load data into spark df, stringIndex all cat_cols\n",
    "\n",
    "Stringindex all -> train test split -> smote train -> restore smoted train to original columns -> train models -> transform testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('datasets/adult.csv')\n",
    "# if directly using spark.read.csv('datasets/adult.csv',header=True), unless we specify schema manually,\n",
    "# all columns will be interpreted as string type, troublesome for later process\n",
    "dataset = util.pandas_to_spark(sqlContext,df)\n",
    "dataset = dataset.withColumn('income', when(dataset.income=='<=50K', lit(0)).otherwise(1))\n",
    "cols = dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34294\n",
      "14548\n"
     ]
    }
   ],
   "source": [
    "trainingData, testData = dataset.randomSplit([0.7, 0.3], seed=100)\n",
    "\n",
    "print(trainingData.count())\n",
    "print(testData.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) Random downsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'lib.imbalance_handler' from '/Users/hwang/Desktop/spark_pipelines/lib/imbalance_handler.py'>"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from config.conf_template import Struct as Section\n",
    "from pyspark.ml.feature import OneHotEncoderEstimator\n",
    "import lib.imbalance_handler as imbalance_handle\n",
    "import lib.feature_selection as fs\n",
    "import lib.categorical_handler as ctgy\n",
    "from pyspark.ml import Pipeline\n",
    "reload(ctgy)\n",
    "reload(fs)\n",
    "reload(imbalance_handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After downsampling \"income\": label distribution is [Row(income=0, count=16501), Row(income=1, count=8251)]\n"
     ]
    }
   ],
   "source": [
    "down_sampled_df = imbalance_handle.spark_df_down_sampling(trainingData, 2, 'income', major_class_val = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Smote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All columns are been covered.\n"
     ]
    }
   ],
   "source": [
    "# get num_cols and cat_cols from spark df\n",
    "num_cols, cat_cols = util.get_num_cat_feat(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_cat = 2\n",
    "max_cat = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start the count computation for categorical features...\n",
      "The no. of categorical features: 8\n"
     ]
    }
   ],
   "source": [
    "cat_coverage_df,no_info_col,cols_high_cardinality = fs.cat_col_cardinality_test(dataset,cat_cols,min_cat,max_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['income', 'hours-per-week', 'educational-num', 'capital-gain', 'age']"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find highly correlated columns\n",
    "fs.num_cols_correlation_test(dataset,num_cols,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = Section(\"smote_config\")\n",
    "conf.seed = 48\n",
    "conf.bucketLength = 100\n",
    "conf.k = 4\n",
    "conf.multiplier = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All columns are been covered.\n",
      "return num cols vectorized df and stages for testset transformation\n",
      "generating batch 0 of synthetic instances\n",
      "generating batch 1 of synthetic instances\n",
      "generating batch 2 of synthetic instances\n"
     ]
    }
   ],
   "source": [
    "# get num_cols and cat_cols from spark df\n",
    "num_cols, cat_cols = util.get_num_cat_feat(dataset)\n",
    "vectorized,stages1 = imbalance_handle.pre_smote_df_process(trainingData,num_cols,cat_cols,'income',False)\n",
    "res = imbalance_handle.smote(vectorized,conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_restored = restore_smoted_df(num_cols,res,'features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Encode categorical cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "stages_rf = ctgy.assemble_into_features_RF(res_restored,num_cols,cat_cols,'_index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with encoding on cat cols\n",
    "allstages = ctgy.assemble_into_features(res_restored,num_cols,cat_cols,'_index','_ohe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "#having compiled the stages into a list, at execution, it will automatically sort out the sequence to perform steps in stages\n",
    "#like when .fit() is called, what should be executed...\n",
    "partialPipeline = Pipeline().setStages(stages_rf) #type is pipeline, independent of dataframe, only using stages \n",
    "pipelineModel = partialPipeline.fit(res_restored) #type is pipelinemodel, use the prepared staged pipelines to transform test dataframe or train\n",
    "preppedDataDF = pipelineModel.transform(res_restored) #type is stage transformed dataframe, it contains all original columns, and indexed/encoded/vector_encoded columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "preppedDataDF_test = pipelineModel.transform(testData)\n",
    "preppedDataDF_test = preppedDataDF_test.withColumnRenamed(\"income\",\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now to train on the train set\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "import numpy as np\n",
    "\n",
    "rf_settings = {'maxBins':100,\n",
    "              'labelCol':'label',\n",
    "              'featuresCol':'features'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building random forest feature selector using maxBins:100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['marital-status_index',\n",
       " 'relationship_index',\n",
       " 'educational-num',\n",
       " 'capital-gain',\n",
       " 'occupation_index',\n",
       " 'education_index',\n",
       " 'gender_index',\n",
       " 'hours-per-week',\n",
       " 'age',\n",
       " 'capital-loss']"
      ]
     },
     "execution_count": 409,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RF_feature_selector(rf_settings,preppedDataDF,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now to train on the train set\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "# Create an initial RandomForest model.\n",
    "rf = RandomForestClassifier(maxBins=100, labelCol=\"label\", featuresCol=\"features\")\n",
    "# Train model with Training Data\n",
    "rfModel = rf.fit(preppedDataDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on test data using the transform() method.\n",
    "# LogisticRegression.transform() will only use the 'features' column.\n",
    "predictions = rfModel.transform(preppedDataDF_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "# Evaluate model\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\n",
    "areaUnderROC = evaluator.setMetricName(\"areaUnderROC\").evaluate(predictions)\n",
    "areaUnderPR = evaluator.setMetricName(\"areaUnderPR\").evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = predictions.select(\"prediction\",\"label\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7433, 3679],\n",
       "       [ 372, 3064]])"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_pred=preds['prediction'],y_true=preds['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for ohe transformed features \n",
    "pandasDF = pd.DataFrame(preppedDataDF.schema[\"features\"].metadata[\"ml_attr\"][\"attrs\"][\"nominal\"]+preppedDataDF.schema[\"features\"].metadata[\"ml_attr\"][\"attrs\"][\"numeric\"]).sort_values(\"idx\")\n",
    "\n",
    "feature_dict = dict(zip(pandasDF[\"idx\"],pandasDF[\"name\"])) \n",
    "feature_dict_broad = sc.broadcast(feature_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "importances = list(np.array(rfModel.featureImportances))\n",
    "\n",
    "col_importance_val = []\n",
    "for i,importance in enumerate(importances):\n",
    "    col_importance_val.append([i,importance])\n",
    "\n",
    "final_sorted_importance = sorted(col_importance_val, key=lambda x: x[1], reverse =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_feature_index = [a[0] for a in final_sorted_importance[:15]]\n",
    "\n",
    "res = []\n",
    "for i,importance in enumerate(importances):\n",
    "    feature_nm = feature_dict[i]\n",
    "    res.append([feature_nm,importance])\n",
    "    \n",
    "sorted_important_fs = sorted(res, key=lambda x: x[1], reverse =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['marital-status_index', 0.4082046991270808],\n",
       " ['relationship_index', 0.1665893438731733],\n",
       " ['educational-num', 0.16535002474366226],\n",
       " ['capital-gain', 0.10703501011498343],\n",
       " ['occupation_index', 0.06260924091269396],\n",
       " ['education_index', 0.04168504655448925],\n",
       " ['gender_index', 0.02210641676730955],\n",
       " ['hours-per-week', 0.01547938921110717],\n",
       " ['capital-loss', 0.005645579417788404],\n",
       " ['age', 0.004702962533155233],\n",
       " ['native-country_index', 0.0003211945125361774],\n",
       " ['workclass_index', 0.0001543171015540678],\n",
       " ['race_index', 7.339355307677681e-05],\n",
       " ['fnlwgt', 4.338157738968021e-05]]"
      ]
     },
     "execution_count": 396,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_important_fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doing cross validation and params tuning\n",
    "\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(rf.maxDepth, [2, 4, 6])\n",
    "             .addGrid(rf.maxBins, [20, 60])\n",
    "             .addGrid(rf.numTrees, [5, 20])\n",
    "             .build())\n",
    "# paramGrid contains 3*2*2 = 12 models\n",
    "# cv is 5 folds, so total 60 models are searched\n",
    "\n",
    "# Create 5-fold CrossValidator, input is an estimator (rf classifier e.g.)\n",
    "cv = CrossValidator(estimator=rf, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=5)\n",
    "# Run cross validations\n",
    "cvModel = cv.fit(trainingData)\n",
    "# this will likely take a fair amount of time because of the amount of models that we're creating and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use test set to measure the accuracy of our model on new data\n",
    "predictions = cvModel.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8999505914844388"
      ]
     },
     "execution_count": 370,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cvModel uses the best model found from the Cross Validation\n",
    "# Evaluate best model\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector assembler can have inputs as: numeric,bool,vector\n",
    "# output will be a flattened vector (even if input could have vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestModel = cvModel.bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions for entire dataset\n",
    "finalPredictions = bestModel.transform(dataset)\n",
    "# Evaluate best model\n",
    "evaluator.evaluate(finalPredictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "from pyspark.ml.feature import StringIndexer, IndexToString\n",
    "labelReverse = IndexToString().setInputCol(\"race_index\").setOutputCol(\"recover\")\n",
    "labelReverse.transform(vectorized).select(\"race_index\",\"recover\").show()\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
