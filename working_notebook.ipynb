{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General pipeline for binary classification tasks in spark ml\n",
    "\n",
    "$\\color{blue}{\\text{Covering major components in real life scenarios for binary classification tasks using structured datasets.}}$\n",
    "\n",
    "Modelling and data transformation part will be mostly operated in pure spark, but exploration and plotting metrics may involve pandas components.\n",
    "\n",
    "The toolkits are in the lib/ folder, covering following topics:\n",
    "\n",
    "0. summary on transformer, estimators, pipelines\n",
    "1. spark and pandas dataframe conversion, tips in converting datatypes and assign correct schema\n",
    "2. typical udf to transform columns\n",
    "3. explorative analysis on spark df\n",
    "4. categorical variables encoding methods, some advanced types of encoding implemented\n",
    "5. feature selection methods in spark ml, selection based on model, lasso...\n",
    "6. handling skewed datasets and highly imbalanced labels (up/down sampling) SMOTE in spark\n",
    "7. modelling toolkits, contains common classifiers and their tuning guidance, use of xgboost in spark\n",
    "8. metrics plotting tools, to plot common metrics after training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding of spark ml structure:\n",
    "Key components:\n",
    "1. Transformer\n",
    "2. Estimator\n",
    "3. Pipeline\n",
    "\n",
    "$\\textbf{Transformer}$ can transform one df into another df by appending new columns onto original df. E.g., an ML model is a Transformer which transforms a DataFrame with features into a DataFrame with predictions. It has .transform() method, normally taking df as input. Transformers can be trained models, trained encoders.\n",
    "\n",
    "$\\textbf{Estimator}$ is an algorithm to be fit on a df to produce a Transformer. E.g., a learning algorithm is an Estimator which trains on a df and produces a model; if we specify a One-hot-encoder, it is an estimarot object, we need to .fit() it onto a column and obtain a transformer. Output of fitted/trained estimator is transformer.\n",
    "\n",
    "$\\textbf{Pipeline}$ chains multiple Transformers and Estimators together to specify an ML workflow. When executing the pipeline, spark will automatically sort out the steps to execute, depending on whether you called a .fit() or .transform() method. A Pipeline is specified as a sequence of stages, and each stage is either a Transformer or an Estimator. These stages are run in order, and the input DataFrame is transformed as it passes through each stage. For Transformer stages, the transform() method is called on the DataFrame. For Estimator stages, the fit() method is called to produce a Transformer (which becomes part of the PipelineModel, or fitted Pipeline). Pipeline is an estimator, after calling pipeline.fit() method, the output will be PipelineModel, a transformer ready to apply .transform() at test time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "pd.options.display.max_columns=None\n",
    "pd.options.display.max_rows=None\n",
    "\n",
    "#import toolkits\n",
    "from lib import util\n",
    "from lib import logger\n",
    "\n",
    "def initialize_spark(app_name='spark_pipeline'):\n",
    "    import findspark\n",
    "    #spark path using default value\n",
    "    findspark.init()\n",
    "\n",
    "    import pyspark\n",
    "    import pyarrow\n",
    "    from pyspark.sql import SQLContext\n",
    "\n",
    "    conf = pyspark.SparkConf()\\\n",
    "        .setAppName(app_name)\\\n",
    "        .setMaster('local')\\\n",
    "        .set('spark.driver.memory', '8g')\\\n",
    "        .set('spark.executor.memory', '8g')\\\n",
    "        .set('spark.executor.instances', 4)\\\n",
    "        .set('spark.executor.cores', 4)\\\n",
    "        .set('spark.driver.maxResultSize', '8g')\\\n",
    "        .set('spark.sql.shuffle.partitions', 100)\\\n",
    "        .set('spark.default.parallelism', 200)\\\n",
    "        .set('spark.sql.broadcastTimeout', 36000)\\ #broadcastTimeout is purposedly set to be large due to development on single machine\n",
    "        .set('spark.kryoserializer.buffer.max', '1024m')\\\n",
    "        .set('spark.sql.execution.arrow.enabled', 'false')\\\n",
    "        .set('spark.dynamicAllocation.enabled', \"False\")\\\n",
    "        .set('spark.port.maxRetries',30) \n",
    "\n",
    "    sc = pyspark.SparkContext.getOrCreate(conf)\n",
    "    spark = pyspark.sql.SparkSession(sc)\n",
    "    sqlContext = SQLContext.getOrCreate(sc)    \n",
    "    return sc,spark,sqlContext\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml.feature import OneHotEncoderEstimator, StringIndexer, VectorAssembler\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import when, lit\n",
    "from distutils.version import LooseVersion\n",
    "from importlib import reload\n",
    "import pyspark.sql.functions as func\n",
    "import pyspark.sql.types as typ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc,spark,sqlContext = initialize_spark()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data into spark dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('datasets/adult.csv')\n",
    "# if directly using spark.read.csv('datasets/adult.csv',header=True), unless we specify schema manually,\n",
    "# all columns will be interpreted as string type, troublesome for later process\n",
    "dataset = util.pandas_to_spark(sqlContext,df)\n",
    "dataset = dataset.withColumn('income', when(dataset.income=='<=50K', lit(0)).otherwise(1))\n",
    "cols = dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'lib.imbalance_handler' from '/Users/hwang/Desktop/spark_pipelines/lib/imbalance_handler.py'>"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lib import imbalance_handler as imbalance_handle\n",
    "reload(imbalance_handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After downsampling \"income\": label distribution is [Row(income=0, count=11687), Row(income=1, count=11687)]\n"
     ]
    }
   ],
   "source": [
    "down_sampled_df = imbalance_handle.spark_df_down_sampling(dataset, 1, 'income', major_class_val = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All columns are been covered.\n"
     ]
    }
   ],
   "source": [
    "num_cols, cat_cols = util.get_num_cat_feat(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_cat = 2\n",
    "max_cat = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start the count computation for categorical features...\n",
      "The no. of categorical features: 8\n"
     ]
    }
   ],
   "source": [
    "#function to automate generate string columns and numerical columns from spark df\n",
    "cat_coverage_df,no_info_col,cols_high_cardinality = util.coverage_test_spark(dataset,cat_cols,min_cat,max_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['educational-num', 'capital-gain']"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lib import feature_selection as f_selector\n",
    "reload(f_selector)\n",
    "\n",
    "f_selector.num_cols_correlation_test(dataset,num_cols,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Smote, encoding, preprocessing, normalization should happen before smote\n",
    "## Example of spark LSH projection to find nearnest neighbours, as a step in smote upsampling\n",
    "BucketedRandomProjectionLSH: LSH class for Euclidean distance metrics. The input is dense or sparse vectors, each of which represents a point in the Euclidean distance space. The output will be vectors of configurable dimension. Hash values in the same dimension are calculated by the same hash function.\n",
    "\n",
    "Input spark df divided into 3 parts:\n",
    "1. LongType discrete attributes\n",
    "2. StringType discrete attributes\n",
    "3. Continuous numerical type attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from pyspark.sql import Row\n",
    "from sklearn import neighbors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "#for categorical columns, must take its stringIndexed form (smote should be after string indexing, default by frequency)\n",
    "#using smote on stringIndexed categorical cols, just take randomly of sample value or it's neighbour's value\n",
    "def __vectorAssembler(df,cols_2_vec,target_col):\n",
    "    if(df.select(target_col).distinct().count() != 2):\n",
    "        raise ValueError(\"Target field must have only 2 distinct classes\")\n",
    "        \n",
    "    if target_col in cols_2_vec:\n",
    "        cols_2_vec.remove(target_col)\n",
    "\n",
    "    #only vectorize desired columns\n",
    "    df = df.select(*(cols_2_vec+[target_col]))\n",
    "    \n",
    "    #only assembled numeric columns\n",
    "    assembler=VectorAssembler(inputCols = cols_2_vec, outputCol = 'features')\n",
    "    \n",
    "    pos_vectorized = assembler.transform(df)\n",
    "    \n",
    "    vectorized = pos_vectorized.select('features',target_col).withColumn('label',pos_vectorized[target_col]).drop(target_col)\n",
    "    \n",
    "    return vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized = __vectorAssembler(dataset,num_cols,'income')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = vectorized.rdd.map(lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#in spark same syntex as pandas to slice df\n",
    "dataInput_min = vectorized[vectorized['label'] == 1]\n",
    "dataInput_maj = vectorized[vectorized['label'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = dataInput_min.select('features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The hashed dataset where hashed values are stored in the column 'hashes':\n",
      "+--------------------+-----+-----------+\n",
      "|            features|label|     hashes|\n",
      "+--------------------+-----+-----------+\n",
      "|[25.0,226802.0,7....|    1|[[10174.0]]|\n",
      "|[38.0,89814.0,9.0...|    1| [[4027.0]]|\n",
      "|[18.0,103497.0,10...|    1| [[4642.0]]|\n",
      "|[34.0,198693.0,6....|    1| [[8913.0]]|\n",
      "|[29.0,227026.0,9....|    1|[[10184.0]]|\n",
      "|[24.0,369667.0,10...|    1|[[16584.0]]|\n",
      "|[55.0,104996.0,4....|    1| [[4710.0]]|\n",
      "|[36.0,212465.0,13...|    1| [[9531.0]]|\n",
      "|[26.0,82091.0,9.0...|    1| [[3681.0]]|\n",
      "|[58.0,299831.0,9....|    1|[[13451.0]]|\n",
      "|[20.0,444554.0,10...|    1|[[19945.0]]|\n",
      "|[43.0,128354.0,9....|    1| [[5757.0]]|\n",
      "|[37.0,60548.0,9.0...|    1| [[2715.0]]|\n",
      "|[34.0,238588.0,10...|    1|[[10703.0]]|\n",
      "|[72.0,132015.0,4....|    1| [[5922.0]]|\n",
      "|[25.0,220931.0,13...|    1| [[9911.0]]|\n",
      "|[25.0,205947.0,13...|    1| [[9239.0]]|\n",
      "|[22.0,236427.0,9....|    1|[[10607.0]]|\n",
      "|[23.0,134446.0,9....|    1| [[6029.0]]|\n",
      "|[54.0,99516.0,9.0...|    1| [[4463.0]]|\n",
      "+--------------------+-----+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import BucketedRandomProjectionLSH\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql.functions import col\n",
    "#fit the model of bucketedrandomprojection\n",
    "brp = BucketedRandomProjectionLSH(inputCol=\"features\", outputCol=\"hashes\",seed=12345, bucketLength=7)\n",
    "model = brp.fit(dataInput_min)\n",
    "# Feature Transformation\n",
    "print(\"The hashed dataset where hashed values are stored in the column 'hashes':\")\n",
    "model.transform(dataInput_min).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximately joining dfA and dfB on Euclidean distance smaller than 1.5:\n",
      "+--------------------+--------------------+-----------------+\n",
      "|            datasetA|            datasetB|EuclideanDistance|\n",
      "+--------------------+--------------------+-----------------+\n",
      "|[[39.0,182828.0,1...|[[39.0,182828.0,1...|              0.0|\n",
      "|[[30.0,182833.0,1...|[[30.0,182833.0,1...|              0.0|\n",
      "|[[48.0,246367.0,1...|[[48.0,246367.0,1...|              0.0|\n",
      "|[[47.0,46857.0,10...|[[47.0,46857.0,10...|              0.0|\n",
      "|[[36.0,73023.0,9....|[[36.0,73023.0,9....|              0.0|\n",
      "|[[42.0,125461.0,1...|[[42.0,125461.0,1...|              0.0|\n",
      "|[[22.0,126613.0,1...|[[21.0,126613.0,1...|              1.0|\n",
      "|[[42.0,335846.0,1...|[[42.0,335846.0,1...|              0.0|\n",
      "|[[26.0,40255.0,11...|[[26.0,40255.0,11...|              0.0|\n",
      "|[[38.0,194809.0,1...|[[38.0,194809.0,1...|              0.0|\n",
      "|[[64.0,201700.0,4...|[[64.0,201700.0,4...|              0.0|\n",
      "|[[30.0,340899.0,1...|[[30.0,340899.0,1...|              0.0|\n",
      "|[[39.0,158956.0,1...|[[39.0,158956.0,1...|              0.0|\n",
      "|[[32.0,178691.0,1...|[[32.0,178691.0,1...|              0.0|\n",
      "|[[35.0,179171.0,9...|[[35.0,179171.0,9...|              0.0|\n",
      "|[[18.0,298860.0,7...|[[18.0,298860.0,7...|              0.0|\n",
      "|[[28.0,215955.0,9...|[[28.0,215955.0,9...|              0.0|\n",
      "|[[74.0,216001.0,4...|[[74.0,216001.0,4...|              0.0|\n",
      "|[[37.0,224566.0,1...|[[37.0,224566.0,1...|              0.0|\n",
      "|[[63.0,445168.0,1...|[[63.0,445168.0,1...|              0.0|\n",
      "+--------------------+--------------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compute the locality sensitive hashes for the input rows, then perform approximate\n",
    "# similarity join.\n",
    "# We could avoid computing hashes by passing in the already-transformed dataset, e.g.\n",
    "# `model.approxSimilarityJoin(transformedA, transformedB, 1.5)`\n",
    "print(\"Approximately joining dfA and dfB on Euclidean distance smaller than 1.5:\")\n",
    "model.approxSimilarityJoin(vectorized, dataInput_min, 1.5, distCol=\"EuclideanDistance\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = Vectors.dense([1.0, 0.0])\n",
    "\n",
    "# Compute the locality sensitive hashes for the input rows, then perform approximate nearest\n",
    "# neighbor search.\n",
    "# We could avoid computing hashes by passing in the already-transformed dataset, e.g.\n",
    "# `model.approxNearestNeighbors(transformedA, key, 2)`\n",
    "\n",
    "print(\"Approximately searching dfA for 2 nearest neighbors of the key:\")\n",
    "model.approxNearestNeighbors(dfA, key, 2).show()\n",
    "\n",
    "#method 2: just use udf to wrap in the model and distributedly compute each minority sample and find their k nearnest neighbours from the original dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import BucketedRandomProjectionLSH\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "data_A = [(0, Vectors.dense([-1.0, -1.0 ]),),\n",
    "        (1, Vectors.dense([-1.0, 1.0 ]),),\n",
    "        (2, Vectors.dense([1.0, -1.0 ]),),\n",
    "        (3, Vectors.dense([1.0, 1.0]),)]\n",
    "dfA = spark.createDataFrame(data_A, [\"id\", \"features\"])\n",
    "\n",
    "data_B = [(4, Vectors.dense([2.0, 2.0 ]),),\n",
    "        (5, Vectors.dense([2.0, 3.0 ]),),\n",
    "        (6, Vectors.dense([3.0, 2.0 ]),),\n",
    "        (7, Vectors.dense([3.0, 3.0]),)]\n",
    "dfB = spark.createDataFrame(data_B, [\"id\", \"features\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit the model of bucketedrandomprojection\n",
    "brp = BucketedRandomProjectionLSH(inputCol=\"features\", outputCol=\"hashes\",seed=12345, bucketLength=7)\n",
    "model = brp.fit(dfA)\n",
    "# Feature Transformation\n",
    "print(\"The hashed dataset where hashed values are stored in the column 'hashes':\")\n",
    "model.transform(dfA).show()\n",
    "\n",
    "# Compute the locality sensitive hashes for the input rows, then perform approximate\n",
    "# similarity join.\n",
    "# We could avoid computing hashes by passing in the already-transformed dataset, e.g.\n",
    "# `model.approxSimilarityJoin(transformedA, transformedB, 1.5)`\n",
    "print(\"Approximately joining dfA and dfB on Euclidean distance smaller than 1.5:\")\n",
    "model.approxSimilarityJoin(dfA, dfB, 1.5, distCol=\"EuclideanDistance\")\\\n",
    "    .select(col(\"datasetA.id\").alias(\"idA\"),\n",
    "            col(\"datasetB.id\").alias(\"idB\"),\n",
    "            col(\"EuclideanDistance\")).show()\n",
    "\n",
    "key = Vectors.dense([1.0, 0.0])\n",
    "\n",
    "# Compute the locality sensitive hashes for the input rows, then perform approximate nearest\n",
    "# neighbor search.\n",
    "# We could avoid computing hashes by passing in the already-transformed dataset, e.g.\n",
    "# `model.approxNearestNeighbors(transformedA, key, 2)`\n",
    "\n",
    "print(\"Approximately searching dfA for 2 nearest neighbors of the key:\")\n",
    "model.approxNearestNeighbors(dfA, key, 2).show()\n",
    "\n",
    "#BAD METHOD: method 1: using approxSimilarityJoin to find all similar point pairs within the given threshold euclidean distance\n",
    "#the euclidean distance will be obtained by monte-carlo method on a few points using approxNearestNeighbors and given desired nearest neighbours\n",
    "\n",
    "#method 2: just use udf to wrap in the model and distributedly compute each minority sample and find their k nearnest neighbours from the original dataframe\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorSlicer\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql.types import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|        userFeatures|            features|\n",
      "+--------------------+--------------------+\n",
      "|(3,[0,1],[-2.0,2.3])|(3,[0,1],[-2.0,2.3])|\n",
      "|      [-2.0,2.3,0.0]|      [-2.0,2.3,0.0]|\n",
      "|(3,[0,1],[-2.0,2.3])|(3,[0,1],[-2.0,2.3])|\n",
      "|(3,[0,1],[-2.0,2.3])|(3,[0,1],[-2.0,2.3])|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Row(userFeatures=Vectors.sparse(3, {0: -2.0, 1: 2.3})),\n",
    "vectors.sparse: first param shows length of this vector, then comes with a dict storing actual index-value pair\n",
    "vectors.dense: input param is a list containing all the information at all positions (thus dense)\n",
    "'''\n",
    "#VectorSlicer is a transformer that takes a feature vector and outputs a new feature vector \n",
    "#with a sub-array of the original features. It is useful for extracting features from a vector column.\n",
    "\n",
    "df = spark.createDataFrame([Row(userFeatures=Vectors.sparse(3, {0: -2.0, 1: 2.3})),\n",
    "    Row(userFeatures=Vectors.dense([-2.0, 2.3, 0.0])),\n",
    "    Row(userFeatures=Vectors.sparse(3, {0: -2.0, 1: 2.3})),\n",
    "    Row(userFeatures=Vectors.sparse(3, {0: -2.0, 1: 2.3}))])\n",
    "\n",
    "#first deal with all numerical features and then keep its length, to be used in vectorslicer \n",
    "\n",
    "slicer = VectorSlicer(inputCol=\"userFeatures\", outputCol=\"features\", indices=[0,1,2])\n",
    "\n",
    "output = slicer.transform(df)\n",
    "\n",
    "output.select(\"userFeatures\", \"features\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "\n",
    "@udf(\"string\")\n",
    "def sum_dense_vector_udf(s):\n",
    "    return str(sum(s))\n",
    "\n",
    "@udf(\"string\")\n",
    "def __find_knn(rowin):\n",
    "    print(rowin)\n",
    "    return str(knn_df.count())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class smote_pyspark():\n",
    "    discreteAttributes\n",
    "    continuousAttributes\n",
    "    bucketLength #length of each bucket\n",
    "    numHashTables #number of hash tables\n",
    "    sizeMultiplier #synthetics per example\n",
    "    numNearestNeighbours\n",
    "    seed\n",
    "    spark\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ChiSquare feature selector on categorical features\n",
    "ChiSqSelector stands for Chi-Squared feature selection. It operates on labeled data with categorical features. ChiSqSelector uses the Chi-Squared test of independence to decide which features to choose. It supports five selection methods: numTopFeatures, percentile, fpr, fdr, fwe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChiSqSelector output with top 1 features selected\n",
      "+---+------------------+-------+----------------+\n",
      "| id|          features|clicked|selectedFeatures|\n",
      "+---+------------------+-------+----------------+\n",
      "|  7|[0.0,0.0,18.0,1.0]|    1.0|          [18.0]|\n",
      "|  8|[0.0,1.0,12.0,0.0]|    0.0|          [12.0]|\n",
      "|  9|[1.0,0.0,15.0,0.1]|    0.0|          [15.0]|\n",
      "+---+------------------+-------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import ChiSqSelector\n",
    "from pyspark.ml.linalg import Vectors\n",
    "'''\n",
    "numTopFeatures, percentile\n",
    "akin to yielding the features with the most predictive power.\n",
    "'''\n",
    "\n",
    "# example code 1\n",
    "# doing chi-square test on categorical cols\n",
    "import pyspark.mllib.linalg as ln\n",
    "\n",
    "for cat in categorical_cols[1:]:\n",
    "    agg = dataset \\\n",
    "    .groupby('race') \\\n",
    "    .pivot(cat) \\\n",
    "    .count()\n",
    "    \n",
    "    agg_rdd = agg \\\n",
    "    .rdd \\\n",
    "    .map(lambda row: (row[1:])) \\\n",
    "    .flatMap(lambda row:[0 if e == None else e for e in row]).collect()\n",
    "    \n",
    "    row_length = len(agg.collect()[0]) - 1\n",
    "    \n",
    "    agg = ln.Matrices.dense(row_length, 2, agg_rdd)\n",
    "    \n",
    "    test = st.Statistics.chiSqTest\n",
    "    (agg)\n",
    "    \n",
    "    print(cat, round(test.pValue, 4))\n",
    "    \n",
    "    \n",
    "\n",
    "# example code 2\n",
    "df = spark.createDataFrame([\n",
    "    (7, Vectors.dense([0.0, 0.0, 18.0, 1.0]), 1.0,),\n",
    "    (8, Vectors.dense([0.0, 1.0, 12.0, 0.0]), 0.0,),\n",
    "    (9, Vectors.dense([1.0, 0.0, 15.0, 0.1]), 0.0,)], [\"id\", \"features\", \"clicked\"])\n",
    "\n",
    "selector = ChiSqSelector(numTopFeatures=1, featuresCol=\"features\",\n",
    "                         outputCol=\"selectedFeatures\", labelCol=\"clicked\")\n",
    "\n",
    "result = selector.fit(df).transform(df)\n",
    "\n",
    "#will choose the most useful column from features (assembled) \n",
    "print(\"ChiSqSelector output with top %d features selected\" % selector.getNumTopFeatures())\n",
    "result.show()\n",
    "\n",
    "# so for categorical features with not too high cardinality, we can one hot encode it and then do chi-squared test of independence to find which top columns\n",
    "# from the OHE sub-vectors to be kept (but how to retain this information at test/scoring time?) Only transform at test time\n",
    "# When there are three or more levels for the predictor, the degree of association \n",
    "# between predictor and outcome can be measured with statistics such as X2 (chi-squared) tests …"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from distutils.version import LooseVersion\n",
    "from pyspark.ml.feature import StandardScaler,MinMaxScaler\n",
    "\n",
    "\n",
    "#indexes each categorical column using the StringIndexer, \n",
    "#and then converts the indexed categories into one-hot encoded variables. \n",
    "#The resulting output has the binary vectors appended to the end of each row.\n",
    "    \n",
    "def one_hot_encode_cat_cols(sdf,cat_cols,label_col=None):\n",
    "    '''\n",
    "    perform one hot encoding for cat_cols \n",
    "    input:\n",
    "    * sdf: spark df\n",
    "    * cat_cols: categorical columns\n",
    "    * stages: as a list\n",
    "    output:\n",
    "    * stages\n",
    "    '''\n",
    "    stages = [] # stages in our Pipeline\n",
    "\n",
    "    for categoricalCol in cat_cols:\n",
    "        # Category Indexing with StringIndexer, will encode to numerical according to frequency, highest frequency will be encoded to 0\n",
    "        # when applying this stringIndexer onto another dataset and encounter missing encoded value, we can throw exception or setHandleInvalid(“skip”)\n",
    "        # like indexer.fit(df1).setHandleInvalid(\"skip\").transform(df2), will remove all rows unable to encode    \n",
    "        stringIndexer = StringIndexer(inputCol=categoricalCol, outputCol=categoricalCol + \"Index\")\n",
    "\n",
    "        # Use OneHotEncoder to convert categorical variables into binary SparseVectors，\n",
    "        # binary sparse vectors like (2,[0],[1.0]) means a vector of length 2 with 1.0 at position 0 and 0 elsewhere.\n",
    "        # spark OHE will automatically drop the last category, you can force it not to drop by dropLast=False\n",
    "        # it omits the final category to break the correlation between features\n",
    "\n",
    "        if LooseVersion(pyspark.__version__) < LooseVersion(\"3.0\"):\n",
    "            from pyspark.ml.feature import OneHotEncoderEstimator\n",
    "            encoder = OneHotEncoderEstimator(inputCols=[stringIndexer.getOutputCol()], outputCols=[categoricalCol + \"classVec\"])\n",
    "        else:\n",
    "            from pyspark.ml.feature import OneHotEncoder\n",
    "            encoder = OneHotEncoder(inputCols=[stringIndexer.getOutputCol()], outputCols=[categoricalCol + \"classVec\"])\n",
    "        # Add stages.  These are not run here, but will run all at once later on.\n",
    "        stages += [stringIndexer, encoder]\n",
    "\n",
    "    #check if input sdf has label col\n",
    "    if label_col == None:\n",
    "        return stages\n",
    "    else:\n",
    "        # Convert label into label indices using the StringIndexer\n",
    "        label_stringIdx = StringIndexer(inputCol = label_col, outputCol = \"label\")\n",
    "        # now stages contains a lot of stringIndexer and oneHotencoder and a label stringindexer\n",
    "        stages += [label_stringIdx]\n",
    "\n",
    "    return stages\n",
    "    \n",
    "def assemble_into_features(sdf,num_cols,cat_cols,stages):\n",
    "    '''\n",
    "    assemble all features into vector\n",
    "    input:\n",
    "    * \n",
    "    '''\n",
    "    # to combine all the feature columns into a single vector column. \n",
    "    # This includes both the numeric columns and the one-hot encoded binary vector columns in our dataset.\n",
    "    # Transform all features into a vector using VectorAssembler\n",
    "    \n",
    "    # keep track of num cols indices for any smote purposes, in dealing with smote on cat cols\n",
    "    num_cols_indices = list(range(len(num_cols)))\n",
    "    \n",
    "    assemblerInputs = num_cols + [c + \"classVec\" for c in cat_cols]\n",
    "\n",
    "    #assemblerInputs stores all necessary (transformed) columns after all the stages\n",
    "    #VectorAssembler only applied to numerical or transformed categorical columns\n",
    "    assembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\n",
    "    stages += [assembler] \n",
    "\n",
    "    # then we apply scaling on the vectorized features, 2 additional params are:\n",
    "    # withStd: True by default. Scales the data to unit standard deviation.\n",
    "    # withMean: False by default. Centers the data with mean before scaling.\n",
    "    scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\",withMean=True)\n",
    "    #scaler = MinMaxScaler(min=0, max=1, inputCol='features', outputCol='features_minmax')\n",
    "\n",
    "    stages += [scaler] \n",
    "    return stages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "stages = one_hot_encode_cat_cols(dataset,cat_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "stages_2 = assemble_into_features(dataset,num_cols,cat_cols,stages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "#having compiled the stages into a list, at execution, it will automatically sort out the sequence to perform steps in stages\n",
    "#like when .fit() is called, what should be executed...\n",
    "partialPipeline = Pipeline().setStages(stages_2) #type is pipeline, independent of dataframe, only using stages \n",
    "\n",
    "pipelineModel = partialPipeline.fit(dataset) #type is pipelinemodel, use the prepared staged pipelines to fit dataframe\n",
    "\n",
    "preppedDataDF = pipelineModel.transform(dataset) #type is stage transformed dataframe, it contains all original columns, and indexed/encoded/vector_encoded columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep relevant columns\n",
    "cols = dataset.columns\n",
    "selectedcols = [\"label\", \"features\"] + cols\n",
    "\n",
    "dataset = preppedDataDF.select(selectedcols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34294\n",
      "14548\n"
     ]
    }
   ],
   "source": [
    "### Randomly split data into training and test sets. set seed for reproducibility\n",
    "(trainingData, testData) = dataset.randomSplit([0.7, 0.3], seed=100)\n",
    "\n",
    "print(trainingData.count())\n",
    "\n",
    "print(testData.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now to train on the train set\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "# Create an initial RandomForest model.\n",
    "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\")\n",
    "\n",
    "# Train model with Training Data\n",
    "rfModel = rf.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on test data using the transform() method.\n",
    "# LogisticRegression.transform() will only use the 'features' column.\n",
    "predictions = rfModel.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[label: double, prediction: double, probability: vector, age: bigint, occupation: string]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# View model's predictions and probabilities of each prediction class\n",
    "# You can select any columns in the above schema to view as well. For example's sake we will choose age & occupation\n",
    "selected = predictions.select(\"label\", \"prediction\", \"probability\", \"age\", \"occupation\")\n",
    "display(selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "# Evaluate model\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'areaUnderROC'"
      ]
     },
     "execution_count": 366,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.getMetricName()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(rf.maxDepth, [2, 4, 6])\n",
    "             .addGrid(rf.maxBins, [20, 60])\n",
    "             .addGrid(rf.numTrees, [5, 20])\n",
    "             .build())\n",
    "# paramGrid contains 3*2*2 = 12 models\n",
    "# cv is 5 folds, so total 60 models are searched\n",
    "\n",
    "# Create 5-fold CrossValidator, input is an estimator (rf classifier e.g.)\n",
    "cv = CrossValidator(estimator=rf, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=5)\n",
    "# Run cross validations\n",
    "cvModel = cv.fit(trainingData)\n",
    "# this will likely take a fair amount of time because of the amount of models that we're creating and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use test set to measure the accuracy of our model on new data\n",
    "predictions = cvModel.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8999505914844388"
      ]
     },
     "execution_count": 370,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cvModel uses the best model found from the Cross Validation\n",
    "# Evaluate best model\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector assembler can have inputs as: numeric,bool,vector\n",
    "# output will be a flattened vector (even if input could have vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestModel = cvModel.bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions for entire dataset\n",
    "finalPredictions = bestModel.transform(dataset)\n",
    "# Evaluate best model\n",
    "evaluator.evaluate(finalPredictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
