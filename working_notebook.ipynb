{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General pipeline framework for binary classification tasks in spark ml\n",
    "\n",
    "$\\color{blue}{\\text{Covering major components in real life scenarios.}}$\n",
    "\n",
    "The toolkits are in the lib/ folder and including following topics:\n",
    "\n",
    "0. summary on transformer, estimators, pipelines\n",
    "1. spark and pandas dataframe conversion, tips in converting datatypes and assign correct schema\n",
    "2. typical udf to transform columns\n",
    "3. explorative analysis on spark df\n",
    "4. categorical variables encoding methods, some advanced types of encoding implemented\n",
    "5. feature selection methods in spark ml, selection based on model, lasso...\n",
    "6. handling skewed datasets and highly imbalanced labels (up/down sampling) SMOTE in spark\n",
    "7. modelling toolkits, contains common classifiers and their tuning guidance, use of xgboost in spark\n",
    "8. metrics plotting tools, to plot common metrics after training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding of spark ml structure:\n",
    "Key components:\n",
    "1. Transformer\n",
    "2. Estimator\n",
    "3. Pipeline\n",
    "\n",
    "$\\textbf{Transformer}$ can transform one df into another df by appending new columns onto original df. E.g., an ML model is a Transformer which transforms a DataFrame with features into a DataFrame with predictions. It has .transform() method, normally taking df as input. Transformers can be trained models, trained encoders.\n",
    "\n",
    "$\\textbf{Estimator}$ is an algorithm to be fit on a df to produce a Transformer. E.g., a learning algorithm is an Estimator which trains on a df and produces a model; if we specify a One-hot-encoder, it is an estimarot object, we need to .fit() it onto a column and obtain a transformer. Output of fitted/trained estimator is transformer.\n",
    "\n",
    "$\\textbf{Pipeline}$ chains multiple Transformers and Estimators together to specify an ML workflow. When executing the pipeline, spark will automatically sort out the steps to execute, depending on whether you called a .fit() or .transform() method. A Pipeline is specified as a sequence of stages, and each stage is either a Transformer or an Estimator. These stages are run in order, and the input DataFrame is transformed as it passes through each stage. For Transformer stages, the transform() method is called on the DataFrame. For Estimator stages, the fit() method is called to produce a Transformer (which becomes part of the PipelineModel, or fitted Pipeline). Pipeline is an estimator, after calling pipeline.fit() method, the output will be PipelineModel, a transformer ready to apply .transform() at test time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_spark(app_name='spark_pipeline'):\n",
    "    import findspark\n",
    "    #spark path using default value\n",
    "    findspark.init()\n",
    "\n",
    "    import pyspark\n",
    "    import pyarrow\n",
    "    from pyspark.sql import SQLContext\n",
    "\n",
    "    conf = pyspark.SparkConf()\\\n",
    "        .setAppName(app_name)\\\n",
    "        .setMaster('local')\\\n",
    "        .set('spark.driver.memory', '8g')\\\n",
    "        .set('spark.executor.memory', '8g')\\\n",
    "        .set('spark.executor.instances', 4)\\\n",
    "        .set('spark.executor.cores', 4)\\\n",
    "        .set('spark.driver.maxResultSize', '8g')\\\n",
    "        .set('spark.sql.shuffle.partitions', 100)\\\n",
    "        .set('spark.default.parallelism', 200)\\\n",
    "        .set('spark.sql.broadcastTimeout', 36000)\\\n",
    "        .set('spark.kryoserializer.buffer.max', '1024m')\\\n",
    "        .set('spark.sql.execution.arrow.enabled', 'false')\\\n",
    "        .set('spark.dynamicAllocation.enabled', \"False\")\\\n",
    "        .set('spark.port.maxRetries',30) \n",
    "\n",
    "    sc = pyspark.SparkContext.getOrCreate(conf)\n",
    "    spark = pyspark.sql.SparkSession(sc)\n",
    "    sqlContext = SQLContext.getOrCreate(sc)    \n",
    "    return sc,spark,sqlContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "pd.options.display.max_columns=None\n",
    "pd.options.display.max_rows=None\n",
    "\n",
    "import pyspark\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml.feature import OneHotEncoderEstimator, StringIndexer, VectorAssembler\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import when, lit\n",
    "from distutils.version import LooseVersion\n",
    "from importlib import reload\n",
    "\n",
    "#import toolkits\n",
    "from lib import util\n",
    "from lib import logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc,spark,sqlContext = initialize_spark()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data into spark dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('datasets/adult.csv')\n",
    "# if directly using spark.read.csv('datasets/adult.csv',header=True), unless we specify schema manually,\n",
    "# all columns will be interpreted as string type, troublesome for later process\n",
    "dataset = util.pandas_to_spark(sqlContext,df)\n",
    "dataset = dataset.withColumn('income', when(dataset.income=='<=50K', lit(1)).otherwise(0))\n",
    "cols = dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as func\n",
    "import pyspark.sql.types as typ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cols = [(col.name, col.dataType) for col in dataset_transformed.schema]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start the count computation for categorical features...\n",
      "The no. of categorical features: 8\n"
     ]
    }
   ],
   "source": [
    "#function to automate generate string columns and numerical columns from spark df\n",
    "x,y,z=util.coverage_test_spark(dataset,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age: \t38.64 \t 13.71\n",
      "fnlwgt: \t189664.13 \t 105604.03\n",
      "educational-num: \t10.08 \t 2.57\n",
      "capital-gain: \t1079.07 \t 7452.02\n",
      "capital-loss: \t87.50 \t 403.00\n",
      "hours-per-week: \t40.42 \t 12.39\n",
      "workclass [('Private', 33906), ('Self-emp-not-inc', 3862), ('Local-gov', 3136), ('?', 2799), ('State-gov', 1981), ('Self-emp-inc', 1695), ('Federal-gov', 1432), ('Without-pay', 21), ('Never-worked', 10)]\n",
      "education [('HS-grad', 15784), ('Some-college', 10878), ('Bachelors', 8025), ('Masters', 2657), ('Assoc-voc', 2061), ('11th', 1812), ('Assoc-acdm', 1601), ('10th', 1389), ('7th-8th', 955), ('Prof-school', 834), ('9th', 756), ('12th', 657), ('Doctorate', 594), ('5th-6th', 509), ('1st-4th', 247), ('Preschool', 83)]\n",
      "marital-status [('Married-civ-spouse', 22379), ('Never-married', 16117), ('Divorced', 6633), ('Separated', 1530), ('Widowed', 1518), ('Married-spouse-absent', 628), ('Married-AF-spouse', 37)]\n",
      "occupation [('Prof-specialty', 6172), ('Craft-repair', 6112), ('Exec-managerial', 6086), ('Adm-clerical', 5611), ('Sales', 5504), ('Other-service', 4923), ('Machine-op-inspct', 3022), ('?', 2809), ('Transport-moving', 2355), ('Handlers-cleaners', 2072), ('Farming-fishing', 1490), ('Tech-support', 1446), ('Protective-serv', 983), ('Priv-house-serv', 242), ('Armed-Forces', 15)]\n",
      "relationship [('Husband', 19716), ('Not-in-family', 12583), ('Own-child', 7581), ('Unmarried', 5125), ('Wife', 2331), ('Other-relative', 1506)]\n",
      "race [('White', 41762), ('Black', 4685), ('Asian-Pac-Islander', 1519), ('Amer-Indian-Eskimo', 470), ('Other', 406)]\n",
      "gender [('Male', 32650), ('Female', 16192)]\n",
      "native-country [('United-States', 43832), ('Mexico', 951), ('?', 857), ('Philippines', 295), ('Germany', 206), ('Puerto-Rico', 184), ('Canada', 182), ('El-Salvador', 155), ('India', 151), ('Cuba', 138), ('England', 127), ('China', 122), ('South', 115), ('Jamaica', 106), ('Italy', 105), ('Dominican-Republic', 103), ('Japan', 92), ('Guatemala', 88), ('Poland', 87), ('Vietnam', 86), ('Columbia', 85), ('Haiti', 75), ('Portugal', 67), ('Taiwan', 65), ('Iran', 59), ('Nicaragua', 49), ('Greece', 49), ('Peru', 46), ('Ecuador', 45), ('France', 38), ('Ireland', 37), ('Thailand', 30), ('Hong', 30), ('Cambodia', 28), ('Trinadad&Tobago', 27), ('Yugoslavia', 23), ('Laos', 23), ('Outlying-US(Guam-USVI-etc)', 23), ('Scotland', 21), ('Honduras', 20), ('Hungary', 19), ('Holand-Netherlands', 1)]\n",
      "income [(1, 37155), (0, 11687)]\n"
     ]
    }
   ],
   "source": [
    "import pyspark.mllib.stat as st\n",
    "import numpy as np\n",
    "\n",
    "#basic stats for numerical cols\n",
    "numericCols = [\"age\", \"fnlwgt\", \"educational-num\", \"capital-gain\", \"capital-loss\", \"hours-per-week\"]\n",
    "\n",
    "numeric_rdd = dataset.select(numericCols).rdd.map(lambda row: [e for e in row])\n",
    "\n",
    "mllib_stats = st.Statistics.colStats(numeric_rdd)\n",
    "\n",
    "for col, m, v in zip(numericCols,\n",
    "    mllib_stats.mean(),\n",
    "    mllib_stats.variance()):\n",
    "    print('{0}: \\t{1:.2f} \\t {2:.2f}'.format(col, m, np.sqrt(v)))\n",
    "    \n",
    "#basic stats for categorical cols\n",
    "categorical_cols = [e for e in dataset.columns if e not in numericCols]\n",
    "\n",
    "categorical_rdd = dataset\\\n",
    "  .select(categorical_cols)\\\n",
    "  .rdd \\\n",
    "  .map(lambda row: [e for e in row])\n",
    "\n",
    "for i, col in enumerate(categorical_cols):\n",
    "    agg = categorical_rdd \\\n",
    "        .groupBy(lambda row: row[i]) \\\n",
    "        .map(lambda row: (row[0], len(row[1])))\n",
    "    \n",
    "    print(col, sorted(agg.collect(),key=lambda el: el[1],reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.mllib.feature as ft\n",
    "import pyspark.mllib.regression as reg\n",
    "\n",
    "hashing = ft.HashingTF(7)\n",
    "\n",
    "births_hashed = births_transformed \\\n",
    "  .rdd \\\n",
    "  .map(lambda row: [\n",
    "      list(hashing.transform(row[1]).toArray())\n",
    "          if col == 'BIRTH_PLACE'\n",
    "          else row[i]\n",
    "      for i, col\n",
    "      in enumerate(features_to_keep)]) \\\n",
    "  .map(lambda row: [[e] if type(e) == int else e\n",
    "          for e in row]) \\\n",
    "  .map(lambda row: [item for sublist in row\n",
    "          for item in sublist]) \\\n",
    "  .map(lambda row: reg.LabeledPoint(\n",
    "      row[0],\n",
    "      ln.Vectors.dense(row[1:]))\n",
    "      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age-to-educational-num: 0.03\n",
      "age-to-capital-gain: 0.08\n",
      "age-to-capital-loss: 0.06\n",
      "age-to-hours-per-week: 0.07\n",
      "educational-num-to-age: 0.03\n",
      "educational-num-to-capital-gain: 0.13\n",
      "educational-num-to-capital-loss: 0.08\n",
      "educational-num-to-hours-per-week: 0.14\n",
      "capital-gain-to-age: 0.08\n",
      "capital-gain-to-educational-num: 0.13\n",
      "capital-gain-to-hours-per-week: 0.08\n",
      "capital-loss-to-age: 0.06\n",
      "capital-loss-to-educational-num: 0.08\n",
      "capital-loss-to-hours-per-week: 0.05\n",
      "hours-per-week-to-age: 0.07\n",
      "hours-per-week-to-educational-num: 0.14\n",
      "hours-per-week-to-capital-gain: 0.08\n",
      "hours-per-week-to-capital-loss: 0.05\n"
     ]
    }
   ],
   "source": [
    "#find multi-colinearlity\n",
    "multicolinearity_thres = 0\n",
    "\n",
    "corrs = st.Statistics.corr(numeric_rdd)\n",
    "\n",
    "for i, el in enumerate(corrs > multicolinearity_thres):\n",
    "    correlated = [(numericCols[j], corrs[i][j]) for j, e in enumerate(el) if e == 1.0 and j != i]\n",
    "    \n",
    "    if len(correlated) > 0:\n",
    "        for e in correlated:\n",
    "            print('{0}-to-{1}: {2:.2f}'.format(numericCols[i], e[0], e[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from pyspark.sql import Row\n",
    "from sklearn import neighbors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "def vectorizerFunction(dataInput, TargetFieldName):\n",
    "    if(dataInput.select(TargetFieldName).distinct().count() != 2):\n",
    "        raise ValueError(\"Target field must have only 2 distinct classes\")\n",
    "        \n",
    "    columnNames = list(dataInput.columns)\n",
    "    columnNames.remove(TargetFieldName)\n",
    "    \n",
    "    dataInput = dataInput.select(*(columnNames+[TargetFieldName]))\n",
    "    \n",
    "    #only assembled numeric columns\n",
    "    assembler=VectorAssembler(inputCols = columnNames, outputCol = 'features')\n",
    "    \n",
    "    pos_vectorized = assembler.transform(dataInput)\n",
    "    \n",
    "    vectorized = pos_vectorized.select('features',TargetFieldName).withColumn('label',pos_vectorized[TargetFieldName]).drop(TargetFieldName)\n",
    "    \n",
    "    return vectorized\n",
    "\n",
    "def SmoteSampling(vectorized, k = 5, minorityClass = 1, majorityClass = 0, percentageOver = 200, percentageUnder = 100):\n",
    "    if(percentageUnder > 100|percentageUnder < 10):\n",
    "        raise ValueError(\"Percentage Under must be in range 10 - 100\");\n",
    "    if(percentageOver < 100):\n",
    "        raise ValueError(\"Percentage Over must be in at least 100\");\n",
    "        \n",
    "    #in spark same syntex as pandas to slice df\n",
    "    dataInput_min = vectorized[vectorized['label'] == minorityClass]\n",
    "    dataInput_maj = vectorized[vectorized['label'] == majorityClass]\n",
    "    \n",
    "    feature = dataInput_min.select('features')\n",
    "    feature = feature.rdd\n",
    "    feature = feature.map(lambda x: x[0])\n",
    "    \n",
    "    #still collected as list not spark smote\n",
    "    feature = feature.collect()\n",
    "\n",
    "    #modified to pure spark\n",
    "    knn = NearestNeighbors(n_neighbors=3, radius=2.0, \n",
    "                       algorithm='brute', metric='euclidean')\n",
    "    knn.fit(samples)\n",
    "\n",
    "    feature = np.asarray(feature)\n",
    "    \n",
    "    #using the dense vectors to fit and find neighbors\n",
    "    nbrs = neighbors.NearestNeighbors(n_neighbors=k, algorithm='auto').fit(feature)\n",
    "    \n",
    "    neighbours =  nbrs.kneighbors(feature)\n",
    "    \n",
    "    gap = neighbours[0]\n",
    "    neighbours = neighbours[1]\n",
    "    \n",
    "    #minority rdd\n",
    "    min_rdd = dataInput_min.drop('label').rdd\n",
    "    \n",
    "    pos_rddArray = min_rdd.map(lambda x : list(x))\n",
    "    pos_ListArray = pos_rddArray.collect()\n",
    "    \n",
    "    min_Array = list(pos_ListArray)\n",
    "    \n",
    "    newRows = []\n",
    "    \n",
    "    nt = len(min_Array)\n",
    "    \n",
    "    nexs = int(percentageOver/100)\n",
    "    \n",
    "    for i in range(nt):\n",
    "        for j in range(nexs):\n",
    "            neigh = random.randint(1,k)\n",
    "            difs = min_Array[neigh][0] - min_Array[i][0]\n",
    "            newRec = (min_Array[i][0]+random.random()*difs)\n",
    "            newRows.insert(0,(newRec))\n",
    "            \n",
    "    newData_rdd = sc.parallelize(newRows)\n",
    "    newData_rdd_new = newData_rdd.map(lambda x: Row(features = x, label = 1))\n",
    "    new_data = newData_rdd_new.toDF()\n",
    "    new_data_minor = dataInput_min.unionAll(new_data)\n",
    "    new_data_major = dataInput_maj.sample(False, (float(percentageUnder)/float(100)))\n",
    "    return new_data_major.unionAll(new_data_minor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11687\n",
      "[28.0,336951.0,12.0,0.0,0.0,40.0]\n"
     ]
    }
   ],
   "source": [
    "vall = SmoteSampling(vectorizerFunction(keep_ds_p_sp, 'income'), k = 5, minorityClass = 0, majorityClass = 1, percentageOver = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    25286\n",
       "0    11687\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 412,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vall.select(\"label\").toPandas()['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from distutils.version import LooseVersion\n",
    "\n",
    "categoricalColumns = [\"workclass\", \"education\", \"marital-status\", \"occupation\", \"relationship\", \"race\", \"gender\", \"native-country\"]\n",
    "stages = [] # stages in our Pipeline, as a list\n",
    "\n",
    "#indexes each categorical column using the StringIndexer, \n",
    "#and then converts the indexed categories into one-hot encoded variables. \n",
    "#The resulting output has the binary vectors appended to the end of each row.\n",
    "    \n",
    "for categoricalCol in categoricalColumns:\n",
    "    # Category Indexing with StringIndexer, will encode to numerical according to frequency, highest frequency will be encoded to 0\n",
    "    # when applying this stringIndexer onto another dataset and encounter missing encoded value, we can throw exception or setHandleInvalid(“skip”)\n",
    "    # like indexer.fit(df1).setHandleInvalid(\"skip\").transform(df2), will remove all rows unable to encode    \n",
    "    stringIndexer = StringIndexer(inputCol=categoricalCol, outputCol=categoricalCol + \"Index\")\n",
    "    \n",
    "    # Use OneHotEncoder to convert categorical variables into binary SparseVectors，\n",
    "    # binary sparse vectors like (2,[0],[1.0]) means a vector of length 2 with 1.0 at position 0 and 0 elsewhere.\n",
    "    # spark OHE will automatically drop the last category, you can force it not to drop by dropLast=False\n",
    "    # it omits the final category to break the correlation between features\n",
    "    \n",
    "    if LooseVersion(pyspark.__version__) < LooseVersion(\"3.0\"):\n",
    "        from pyspark.ml.feature import OneHotEncoderEstimator\n",
    "        encoder = OneHotEncoderEstimator(inputCols=[stringIndexer.getOutputCol()], outputCols=[categoricalCol + \"classVec\"])\n",
    "    else:\n",
    "        from pyspark.ml.feature import OneHotEncoder\n",
    "        encoder = OneHotEncoder(inputCols=[stringIndexer.getOutputCol()], outputCols=[categoricalCol + \"classVec\"])\n",
    "    # Add stages.  These are not run here, but will run all at once later on.\n",
    "    stages += [stringIndexer, encoder]\n",
    "\n",
    "# Convert label into label indices using the StringIndexer\n",
    "label_stringIdx = StringIndexer(inputCol=\"income\", outputCol=\"label\")\n",
    "#now stages contains a lot of stringIndexer and oneHotencoder and a label stringindexer\n",
    "stages += [label_stringIdx]\n",
    "\n",
    "# to combine all the feature columns into a single vector column. \n",
    "# This includes both the numeric columns and the one-hot encoded binary vector columns in our dataset.\n",
    "# Transform all features into a vector using VectorAssembler\n",
    "numericCols = [\"age\", \"fnlwgt\", \"educational-num\", \"capital-gain\", \"capital-loss\", \"hours-per-week\"]\n",
    "assemblerInputs = [c + \"classVec\" for c in categoricalColumns] + numericCols\n",
    "\n",
    "#assemblerInputs stores all necessary (transformed) columns after all the stages\n",
    "#VectorAssembler only applied to numerical or transformed categorical columns\n",
    "assembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\n",
    "\n",
    "stages += [assembler] \n",
    "\n",
    "# then we apply scaling on the vectorized features, 2 additional params are:\n",
    "# withStd: True by default. Scales the data to unit standard deviation.\n",
    "# withMean: False by default. Centers the data with mean before scaling.\n",
    "from pyspark.ml.feature import StandardScaler,MinMaxScaler\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\",withMean=True)\n",
    "#scaler = MinMaxScaler(min=0, max=1, inputCol='features', outputCol='features_minmax')\n",
    "\n",
    "stages += [scaler] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "#having compiled the stages into a list, at execution, it will automatically sort out the sequence to perform steps in stages\n",
    "#like when .fit() is called, what should be executed...\n",
    "partialPipeline = Pipeline().setStages(stages) #type is pipeline, independent of dataframe, only using stages \n",
    "\n",
    "pipelineModel = partialPipeline.fit(dataset) #type is pipelinemodel, use the prepared staged pipelines to fit dataframe\n",
    "\n",
    "preppedDataDF = pipelineModel.transform(dataset) #type is stage transformed dataframe, it contains all original columns, and indexed/encoded/vector_encoded columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep relevant columns\n",
    "cols = dataset.columns\n",
    "selectedcols = [\"label\", \"features\"] + cols\n",
    "\n",
    "dataset = preppedDataDF.select(selectedcols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34294\n",
      "14548\n"
     ]
    }
   ],
   "source": [
    "### Randomly split data into training and test sets. set seed for reproducibility\n",
    "(trainingData, testData) = dataset.randomSplit([0.7, 0.3], seed=100)\n",
    "\n",
    "print(trainingData.count())\n",
    "\n",
    "print(testData.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now to train on the train set\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "# Create an initial RandomForest model.\n",
    "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\")\n",
    "\n",
    "# Train model with Training Data\n",
    "rfModel = rf.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on test data using the transform() method.\n",
    "# LogisticRegression.transform() will only use the 'features' column.\n",
    "predictions = rfModel.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[label: double, prediction: double, probability: vector, age: bigint, occupation: string]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# View model's predictions and probabilities of each prediction class\n",
    "# You can select any columns in the above schema to view as well. For example's sake we will choose age & occupation\n",
    "selected = predictions.select(\"label\", \"prediction\", \"probability\", \"age\", \"occupation\")\n",
    "display(selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "# Evaluate model\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'areaUnderROC'"
      ]
     },
     "execution_count": 366,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.getMetricName()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(rf.maxDepth, [2, 4, 6])\n",
    "             .addGrid(rf.maxBins, [20, 60])\n",
    "             .addGrid(rf.numTrees, [5, 20])\n",
    "             .build())\n",
    "# paramGrid contains 3*2*2 = 12 models\n",
    "# cv is 5 folds, so total 60 models are searched\n",
    "\n",
    "# Create 5-fold CrossValidator, input is an estimator (rf classifier e.g.)\n",
    "cv = CrossValidator(estimator=rf, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=5)\n",
    "# Run cross validations\n",
    "cvModel = cv.fit(trainingData)\n",
    "# this will likely take a fair amount of time because of the amount of models that we're creating and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use test set to measure the accuracy of our model on new data\n",
    "predictions = cvModel.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8999505914844388"
      ]
     },
     "execution_count": 370,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cvModel uses the best model found from the Cross Validation\n",
    "# Evaluate best model\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector assembler can have inputs as: numeric,bool,vector\n",
    "# output will be a flattened vector (even if input could have vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestModel = cvModel.bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions for entire dataset\n",
    "finalPredictions = bestModel.transform(dataset)\n",
    "# Evaluate best model\n",
    "evaluator.evaluate(finalPredictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doing chi-square test on categorical cols\n",
    "import pyspark.mllib.linalg as ln\n",
    "\n",
    "for cat in categorical_cols[1:]:\n",
    "    agg = dataset \\\n",
    "    .groupby('race') \\\n",
    "    .pivot(cat) \\\n",
    "    .count()\n",
    "    \n",
    "    agg_rdd = agg \\\n",
    "    .rdd \\\n",
    "    .map(lambda row: (row[1:])) \\\n",
    "    .flatMap(lambda row:[0 if e == None else e for e in row]).collect()\n",
    "    \n",
    "    row_length = len(agg.collect()[0]) - 1\n",
    "    \n",
    "    agg = ln.Matrices.dense(row_length, 2, agg_rdd)\n",
    "    \n",
    "    test = st.Statistics.chiSqTest(agg)\n",
    "    \n",
    "    print(cat, round(test.pValue, 4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
